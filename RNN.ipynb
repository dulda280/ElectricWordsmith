{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Andre 3000_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\ASAP Rocky_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\business.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Drake_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\eminemlyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\getIntoIt.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Isaiah Rashad_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\J Cole_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Logic_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Mac Miller_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\melt_session.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Nas_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Royce Da 59_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Tyler The Creator_lyrics.txt\n"
     ]
    }
   ],
   "source": [
    "## from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import os\n",
    "import io\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "\n",
    "\n",
    "#\n",
    "# with open(f\"{cwd}\\inspiration\\{dataroot[0]}\").read().replace('\\n', '. ') as file:\n",
    "#     print(file)\n",
    "cwd = os.getcwd()\n",
    "dataroot = os.listdir(cwd + \"\\inspiration\")\n",
    "\n",
    "sentData = []\n",
    "\n",
    "for i in range(0, len(dataroot)):\n",
    "    print(f\"{cwd}\\inspiration\\{dataroot[i]}\")\n",
    "    data2 = open(f\"{cwd}\\inspiration\\{dataroot[i]}\", encoding=\"utf8\").read().replace(',', '').lower().split('\\n')\n",
    "    sentData.append(data2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"you 'bout to witness hip hop in its most purest\", 'most rawest form flow almost flawless', 'most hardest most honest known artist', 'chip off the old block but old doc is', 'back!', 'looks like batman brought his own robin', \"oh god saddam's got his own laden\", 'with his own private plane his own pilot', 'set to blow college dorm room doors off their hinges', 'oranges peach pears plums syringes', \"yeah here i come i'm inches\", 'away from you dear fear none', 'hip hop is in a state of nine-one-one so', \"let's get down to business\", \"i don't got no time to play around what is this?\", \"must be a circus in town let's shut that shit down\", 'on these clowns can i getta witness?', '(hell yeah!)', \"let's get down to business\", \"i don't got no time to play around what is this?\", \"must be a circus in town let's shut that shit down\", 'on these clowns can i getta witness?', '(hell yeah!)', 'quick gotta move fast gotta perform miracles', 'gee wilikers dre holy bat syllables', 'look at all the bullshit that goes on in gotham', \"when i'm gone time to get rid of these rap criminals\", 'so skip to your lou while i do what i do best', \"you ain't even impressed no more you're used to it\", 'flows too wet nobody close to it', 'nobody says it but still everyone knows the shit', 'the most hated on out of all those who say they get hated on', 'in eighty songs and exaggerate it all so much', \"they make it all up there's no such thing\", 'like a female with good looks who cooks and cleans', 'it just means so much more to so much more', \"people when you're rappin' and you know what for\", \"the show must go on so i'd like to welcome y'all\", \"to marshall and andr√©'s carnival c'mon!\", \"now let's get down to business\", \"i don't got no time to play around what is this?\", \"must be a circus in town let's shut that shit down\", 'on these clowns can i getta witness?', \"let's get down to business\", \"i don't got no time to play around what is this?\", \"must be a circus in town let's shut that shit down\", 'on these clowns can i getta witness?', '(hell yeah!)', \"it's just like old times the dynamic duo\", 'two old friends why panic?', \"you already know who's fully capable the two caped heroes\", 'dial straight down the center 8-0-0', 'you can even call collect the most feared duet', 'since me and elton played career russian roulette', 'and never even seen me blink or get to busting a sweat', 'people stepping over people just to rush to the set', 'just to get to see an mc who breathes so freely', 'ease over these beats and be so breezy', 'jesus how can shit be so easy', 'how can one chandra be so levy', \"turn on these beats mc's don't see me\", 'believe me bet and mtv', \"are gonna grieve when we leave dawg fo' sheezy\", \"can't leave rap alone the game needs me\", 'till we grow beards get weird and disappear', 'into the mountains nothing but clowns down here', \"but we ain't fucking around round here\", 'yo dre!', '(what up?)', 'can i get a hell yeah?', \"let's get down to business\", \"i don't got no time to play around what is this?\", \"must be a circus in town let's shut that shit down\", 'on these clowns can i getta witness?', '(hell yeah!)', \"let's get down to business\", \"i don't got no time to play around what is this?\", \"must be a circus in town let's shut that shit down\", 'on these clowns can i getta witness?', '(hell yeah!)']\n"
     ]
    }
   ],
   "source": [
    "print(sentData[2][:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "textAsList = []\n",
    "textAsList.clear()\n",
    "\n",
    "def extractTextAsList(text):\n",
    "    global textAsList\n",
    "    textAsList += [w for w in text.split(' ') if w.strip() !='' or w=='\\n' or w=='.']\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm\", \"beginnin'\", 'to', 'feel', 'like', 'a', 'rap', 'god', 'rap', 'god']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "extractTextAsList(sentData[4][0])\n",
    "print(textAsList)\n",
    "len(textAsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of words:  391378\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "for index in range(0, len(sentData)):\n",
    "    for j in range(0, len(sentData[index])):\n",
    "        extractTextAsList(sentData[index][j])\n",
    "        \n",
    "print(\"Total amount of words: \", len(textAsList))\n",
    "print(textAsList[11330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24534\n",
      "How many fucks?: 2119\n",
      "How many nibbas?:  2713\n"
     ]
    }
   ],
   "source": [
    "frequencies = {}\n",
    "frequencies.clear()\n",
    "\n",
    "\n",
    "for word in textAsList:\n",
    "    frequencies[word] = frequencies.get(word, 0) + 1\n",
    "    \n",
    "print(len(frequencies))\n",
    "\n",
    "print(\"How many fucks?:\", frequencies['fuck']+ frequencies['fucking'] + frequencies['fucker'] + frequencies['fucks'])\n",
    "print(\"How many nibbas?: \", frequencies['nigga'] + frequencies['niggers'] + frequencies['niggas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with less than 7 appearances: 20588\n",
      "Words with more than 7 appearances: 3946\n",
      "Valid sequences of size 5: 233032\n"
     ]
    }
   ],
   "source": [
    "uncommonWords = set()\n",
    "minFreq = 7\n",
    "minSeq = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "uncommonWords = set([key for key in frequencies.keys() if frequencies[key] < minFreq])\n",
    "words = sorted(set([key for key in frequencies.keys() if frequencies[key] >= minFreq]))\n",
    "\n",
    "numWords = len(words)\n",
    "\n",
    "wordIndices = dict((w, i) for i, w in enumerate(words))\n",
    "indicesWord = dict((i, w) for i, w in enumerate(words))\n",
    "\n",
    "#print(wordIndices)\n",
    "#print(indicesWord)\n",
    "\n",
    "print(f'Words with less than {minFreq} appearances: {len(uncommonWords)}')\n",
    "\n",
    "print(f'Words with more than {minFreq} appearances: {len(words)}')\n",
    "\n",
    "validSeqs = []\n",
    "endSeqWords = []\n",
    "\n",
    "for i in range(len(textAsList) - minSeq):\n",
    "    endSlice = i + minSeq + 1\n",
    "    \n",
    "    if len(set(textAsList[i: endSlice]).intersection(uncommonWords)) == 0:\n",
    "        validSeqs.append(textAsList[i: i + minSeq])\n",
    "        endSeqWords.append(textAsList[i + minSeq])\n",
    "        \n",
    "print(f'Valid sequences of size {minSeq}: {len(validSeqs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['with', 'a', 'truck', 'ha', 'get'], ['more', 'i', \"can't\", 'even', 'feel'], ['stacked', 'my', 'shit', 'list', 'to']]\n"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(validSeqs, endSeqWords, test_size=0.02, random_state=42)\n",
    "\n",
    "print(xTrain[2:5])\n",
    "\n",
    "# Data generator for fit and evaluate\n",
    "def generator(sentenceList, nextWordList, batchSize):\n",
    "    index = 0\n",
    "    \n",
    "    while True:\n",
    "        x = np.zeros((batchSize, minSeq), dtype=np.int32)\n",
    "        y = np.zeros((batchSize), dtype=np.int32)\n",
    "        \n",
    "        for i in range(batchSize):\n",
    "            for t, w in enumerate(sentenceList[index % len(sentenceList)]):\n",
    "                x[i, t] = wordIndices[w]\n",
    "                \n",
    "            y[i] = wordIndices[nextWordList[index % len(sentenceList)]]\n",
    "            \n",
    "            index = index + 1\n",
    "            \n",
    "        yield x, y\n",
    "\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    #helper function to sample and index from a probabillity array\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    \n",
    "    expPreds = np.exp(preds)\n",
    "    \n",
    "    preds = exp_preds / np.sum(expPreds)\n",
    "    \n",
    "    probabIndx = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probabIndx)\n",
    "\n",
    "def onEpochEnd(epoch, logs):\n",
    "    \n",
    "    #Function invoked at end of each epoch. Prints generated text.\n",
    "    fxFile.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "    \n",
    "    #Pick seed at random index\n",
    "    seedIndex = np.random.randint(len(xTrain+xTest))\n",
    "    seed = (xTrain+xTest)[seedIndex]\n",
    "    \n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        \n",
    "        fxFile.write('----- Diversity: ' + str(diversity) + '\\n')\n",
    "        fxFile.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        fxFile.write(' '.join(sentence))\n",
    "        \n",
    "        for i in range(50):\n",
    "            xPred = np.zeros((1, minSeq))\n",
    "            \n",
    "            for t, word in enumerate(sentence):\n",
    "                \n",
    "                xPred[0, t] = wordIndices[word]\n",
    "                \n",
    "            preds = model.predict(xPred, verbose=0)[0]\n",
    "            \n",
    "            nextIndex = sample(preds, diversity)\n",
    "            \n",
    "            nextWord = indicesWord[nextIndex]\n",
    "            \n",
    "            sentence = sentence[1:]\n",
    "            \n",
    "            sentence.append(nextWord)\n",
    "            \n",
    "            fxFile.write('\\n')\n",
    "        \n",
    "        fxFile.write('='*80 + '\\n')\n",
    "        fxFile.flush()\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "\n",
    "def get_model():\n",
    "    print('Build model...')\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    \n",
    "    model.add(Dense(len(words)))\n",
    "    \n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "\"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n",
    "(len(words), minSeq, minFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 145/7283 [..............................] - ETA: 14:42 - loss: 6.9937 - accuracy: 0.0315"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nIndexError: list index out of range\nTraceback (most recent call last):\n\n  File \"C:\\Users\\sebbe\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\sebbe\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\sebbe\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\sebbe\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-145-9ec5815e5f8d>\", line 17, in generator\n    y[i] = wordIndices[nextWordList[index % len(sentenceList)]]\n\nIndexError: list index out of range\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_31468]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-962811da2cea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfxFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'examples.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m model.fit(generator(xTrain, yTest, BATCH_SIZE),\n\u001b[0m\u001b[0;32m     12\u001b[0m          \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidSeqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Graph execution error:\n\nIndexError: list index out of range\nTraceback (most recent call last):\n\n  File \"C:\\Users\\sebbe\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\sebbe\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\sebbe\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\sebbe\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 830, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-145-9ec5815e5f8d>\", line 17, in generator\n    y[i] = wordIndices[nextWordList[index % len(sentenceList)]]\n\nIndexError: list index out of range\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_31468]"
     ]
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "print_callback = keras.callbacks.LambdaCallback(on_epoch_end=onEpochEnd)\n",
    "\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "\n",
    "callbackList = [checkpoint, print_callback, earlyStopping]\n",
    "\n",
    "fxFile = open('examples.txt', 'w')\n",
    "\n",
    "model.fit(generator(xTrain, yTest, BATCH_SIZE),\n",
    "         steps_per_epoch=int(len(validSeqs)/BATCH_SIZE)+1,\n",
    "         epochs=20,\n",
    "         callbacks=callbackList,\n",
    "         validation_data=generator(xTest, yTrain, BATCH_SIZE),\n",
    "         validation_steps=int(len(yTrain)/BATCH_SIZE)+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
