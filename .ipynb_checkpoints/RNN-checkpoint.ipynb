{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Andre 3000_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\ASAP Rocky_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\business.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Drake_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\eminemlyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\getIntoIt.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Isaiah Rashad_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\J Cole_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Logic_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Mac Miller_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\melt_session.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Nas_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Royce Da 59_lyrics.txt\n",
      "C:\\Users\\sebbe\\Desktop\\MED-local\\ElectricWordsmith\\inspiration\\Tyler The Creator_lyrics.txt\n"
     ]
    }
   ],
   "source": [
    "## from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#\n",
    "# with open(f\"{cwd}\\inspiration\\{dataroot[0]}\").read().replace('\\n', '. ') as file:\n",
    "#     print(file)\n",
    "cwd = os.getcwd()\n",
    "dataroot = os.listdir(cwd + \"\\inspiration\")\n",
    "\n",
    "sentData = []\n",
    "\n",
    "for i in range(0, len(dataroot)):\n",
    "    print(f\"{cwd}\\inspiration\\{dataroot[i]}\")\n",
    "    data2 = open(f\"{cwd}\\inspiration\\{dataroot[i]}\", encoding=\"utf8\").read().replace('\\n', '. ').lower()\n",
    "    sentData.append(nltk.sent_tokenize(data2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"you 'bout to witness hip hop in its most purest.\", 'most rawest form, flow almost flawless.', 'most hardest, most honest known artist.', 'chip off the old block but old doc is.', 'back!.', 'looks like batman brought his own robin.', \"oh god, saddam's got his own laden.\", 'with his own private plane, his own pilot.', 'set to blow college dorm room doors off their hinges.', 'oranges, peach, pears, plums, syringes.', \"yeah, here i come, i'm inches.\", 'away from you, dear fear none.', 'hip hop is in a state of nine-one-one, so.', \"let's get down to business.\", \"i don't got no time to play around, what is this?.\", \"must be a circus in town, let's shut that shit down.\", 'on these clowns, can i getta witness?.', '(hell yeah!).', \"let's get down to business.\", \"i don't got no time to play around, what is this?.\", \"must be a circus in town, let's shut that shit down.\", 'on these clowns, can i getta witness?.', '(hell yeah!).', 'quick gotta move fast, gotta perform miracles.', 'gee wilikers dre, holy bat syllables.', 'look at all the bullshit that goes on in gotham.', \"when i'm gone time to get rid of these rap criminals.\", 'so skip to your lou, while i do what i do best.', \"you ain't even impressed no more, you're used to it.\", 'flows too wet, nobody close to it.', 'nobody says it, but still everyone knows the shit.', 'the most hated on out of all those who say they get hated on.', 'in eighty songs and exaggerate it all so much.', \"they make it all up, there's no such thing.\", 'like a female with good looks, who cooks and cleans.', 'it just means so much more to so much more.', \"people, when you're rappin' and you know what for.\", \"the show must go on, so i'd like to welcome y'all.\", \"to marshall and andr√©'s carnival, c'mon!.\", \"now let's get down to business.\", \"i don't got no time to play around, what is this?.\", \"must be a circus in town, let's shut that shit down.\", 'on these clowns, can i getta witness?.', \"let's get down to business.\", \"i don't got no time to play around, what is this?.\", \"must be a circus in town, let's shut that shit down.\", 'on these clowns, can i getta witness?.', '(hell yeah!).', \"it's just like old times, the dynamic duo.\", 'two old friends, why panic?.', \"you already know who's fully capable, the two caped heroes.\", 'dial straight down the center 8-0-0. you can even call collect, the most feared duet.', 'since me and elton, played career russian roulette.', 'and never even seen me blink or get to busting a sweat.', 'people stepping over people just to rush to the set.', 'just to get to see an mc who breathes so freely.', 'ease over these beats and be so breezy.', 'jesus, how can shit be so easy.', 'how can one chandra be so levy.', \"turn on these beats mc's don't see me.\", 'believe me, bet and mtv.', \"are gonna grieve when we leave, dawg fo' sheezy.\", \"can't leave rap alone the game needs me.\", 'till we grow beards, get weird and disappear.', 'into the mountains, nothing but clowns down here.', \"but we ain't fucking around round here.\", 'yo dre!.', '(what up?).', 'can i get a hell yeah?.', \"let's get down to business.\", \"i don't got no time to play around, what is this?.\", \"must be a circus in town, let's shut that shit down.\", 'on these clowns, can i getta witness?.', '(hell yeah!).', \"let's get down to business.\", \"i don't got no time to play around, what is this?.\", \"must be a circus in town, let's shut that shit down.\", 'on these clowns, can i getta witness?.', '(hell yeah!)']\n"
     ]
    }
   ],
   "source": [
    "print(sentData[2][:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "textAsList = []\n",
    "textAsList.clear()\n",
    "\n",
    "def extractTextAsList(text):\n",
    "    global textAsList\n",
    "    textAsList += [w for w in text.split(' ') if w.strip() !='']\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', \"'bout\", 'to', 'witness', 'hip', 'hop', 'in', 'its', 'most', 'purest.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "extractTextAsList(sentData[2][0])\n",
    "print(textAsList)\n",
    "len(textAsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of words:  394780\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "for index in range(0, len(sentData)):\n",
    "    for j in range(0, len(sentData[index])):\n",
    "        extractTextAsList(sentData[index][j])\n",
    "        \n",
    "print(\"Total amount of words: \", len(textAsList))\n",
    "print(textAsList[11330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34863\n",
      "How many fucks?: 1937\n",
      "How many nibbas?:  1812\n"
     ]
    }
   ],
   "source": [
    "frequencies = {}\n",
    "frequencies.clear()\n",
    "\n",
    "\n",
    "for word in textAsList:\n",
    "    frequencies[word] = frequencies.get(word, 0) + 1\n",
    "    \n",
    "print(len(frequencies))\n",
    "\n",
    "print(\"How many fucks?:\", frequencies['fuck']+ frequencies['fucking'] + frequencies['fucker'] + frequencies['fucks'])\n",
    "print(\"How many nibbas?: \", frequencies['nigga'] + frequencies['niggers'] + frequencies['niggas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with less than 7 appearances: 30288\n",
      "Words with more than 7 appearances: 4575\n",
      "Valid sequences of size 5: 185723\n"
     ]
    }
   ],
   "source": [
    "uncommonWords = set()\n",
    "minFreq = 7\n",
    "minSeq = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "uncommonWords = set([key for key in frequencies.keys() if frequencies[key] < minFreq])\n",
    "words = sorted(set([key for key in frequencies.keys() if frequencies[key] >= minFreq]))\n",
    "\n",
    "numWords = len(words)\n",
    "\n",
    "wordIndices = dict((word, indice) for indice, word in enumerate(words))\n",
    "indicesWord = dict((indice, word) for word, indice in enumerate(words))\n",
    "\n",
    "#print(wordIndices)\n",
    "#print(indicesWord)\n",
    "\n",
    "print(f'Words with less than {minFreq} appearances: {len(uncommonWords)}')\n",
    "\n",
    "print(f'Words with more than {minFreq} appearances: {len(words)}')\n",
    "\n",
    "validSeqs = []\n",
    "endSeqWords = []\n",
    "\n",
    "for i in range(len(textAsList) - minSeq):\n",
    "    endSlice = i + minSeq + 1\n",
    "    \n",
    "    if len(set(textAsList[i: endSlice]).intersection(uncommonWords)) == 0:\n",
    "        validSeqs.append(textAsList[i: i + minSeq])\n",
    "        endSeqWords.append(textAsList[i + minSeq])\n",
    "        \n",
    "print(f'Valid sequences of size {minSeq}: {len(validSeqs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['eat', 'her', 'out', 'with', 'a'], ['made', 'it', 'out', 'the', 'stash'], ['me', 'like', 'you', 'in', 'the']]\n"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(validSeqs, endSeqWords, test_size=0.02, random_state=42)\n",
    "\n",
    "print(xTrain[2:5])\n",
    "\n",
    "# Data generator for fit and evaluate\n",
    "def generator(sentenceList, nextWordList, batchSize):\n",
    "    index = 0\n",
    "    \n",
    "    while True:\n",
    "        x = np.zeros((batchSize, minSeq), dtype=np.int32)\n",
    "        y = np.zeros((batchSize), dtype=np.int32)\n",
    "        \n",
    "        for i in range(batchSize):\n",
    "            for t, w in enumerate(sentenceList[index % len(sentenceList)]):\n",
    "                x[i, t] = wordIndices[w]\n",
    "                \n",
    "            y[i] = wordIndices[nextWordList[index % len(sentenceList)]]\n",
    "            \n",
    "            index += 1\n",
    "            \n",
    "        yield x, y\n",
    "\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    #helper function to sample and index from a probabillity array\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    \n",
    "    expPreds = np.exp(preds)\n",
    "    \n",
    "    preds = exp_preds / np.sum(expPreds)\n",
    "    \n",
    "    probabIndx = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probabIndx)\n",
    "\n",
    "def onEpochEnd(epoch, logs):\n",
    "    \n",
    "    #Function invoked at end of each epoch. Prints generated text.\n",
    "    fxFile.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "    \n",
    "    #Pick seed at random index\n",
    "    seedIndex = np.random.randint(len(xTrain+xTest))\n",
    "    seed = (xTrain+xTest)[seedIndex]\n",
    "    \n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        \n",
    "        fxFile.write('----- Diversity: ' + str(diversity) + '\\n')\n",
    "        fxFile.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        fxFile.write(' '.join(sentence))\n",
    "        \n",
    "        for i in range(50):\n",
    "            xPred = np.zeros((1, minSeq))\n",
    "            \n",
    "            for t, word in enumerate(sentence):\n",
    "                \n",
    "                xPred[0, t] = wordIndices[word]\n",
    "                \n",
    "            preds = model.predict(xPred, verbose=0)[0]\n",
    "            \n",
    "            nextIndex = sample(preds, diversity)\n",
    "            \n",
    "            nextWord = indicesWord[nextIndex]\n",
    "            \n",
    "            sentence = sentence[1:]\n",
    "            \n",
    "            sentence.append(nextWord)\n",
    "            \n",
    "            fxFile.write('\\n')\n",
    "        \n",
    "        fxFile.write('='*80 + '\\n')\n",
    "        fxFile.flush()\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "\n",
    "def get_model():\n",
    "    print('Build model...')\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    \n",
    "    model.add(Dense(len(words)))\n",
    "    \n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "\"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n",
    "(len(words), minSeq, minFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batchSize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-f4ad315d321e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfxFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'examples.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m model.fit(generator(xTrain, yTest, batchSize),\n\u001b[0m\u001b[0;32m     12\u001b[0m          \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidSeqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batchSize' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "print_callback = keras.callbacks.LambdaCallback(on_epoch_end=onEpochEnd)\n",
    "\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "\n",
    "callbackList = [checkpoint, print_callback, earlyStopping]\n",
    "\n",
    "fxFile = open('examples.txt', 'w')\n",
    "\n",
    "model.fit(generator(xTrain, yTest, batchSize),\n",
    "         steps_per_epoch=int(len(validSeqs)/batchSize)+1,\n",
    "         epochs=40,\n",
    "         callbacks=callbackList,\n",
    "         validation_data=generator(xTest, yTrain, batchSize),\n",
    "         validation_steps=int(len(yTrain)/batchSize)+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
